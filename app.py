# app.py
import streamlit as st
import pandas as pd
import pickle
import numpy as np
import os
import faiss
from helper_functions import (
    expand_query_gpt,
    encode_query,
    rerank_results_v13,
    match_synonyms,
    merge_faiss_and_synonym_results,
)

# ğŸ”§ FAISSãƒ»ãƒ™ã‚¯ãƒˆãƒ«ãƒ»ã‚¢ã‚»ãƒƒãƒˆã®å¾©å…ƒ
def restore_faiss_index_from_parts():
    parts = ["faiss_index_part_a", "faiss_index_part_b"]
    if not os.path.exists("faiss_index.index"):
        with open("faiss_index.index", "wb") as f_out:
            for part in parts:
                with open(part, "rb") as f_in:
                    f_out.write(f_in.read())

def restore_meddra_embeddings_from_parts():
    parts = ["meddra_embeddings_part_a", "meddra_embeddings_part_b"]
    if not os.path.exists("meddra_embeddings.npy"):
        with open("meddra_embeddings.npy", "wb") as f_out:
            for part in parts:
                with open(part, "rb") as f_in:
                    f_out.write(f_in.read())

# ğŸ” åˆæœŸãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def load_data():
    restore_faiss_index_from_parts()
    restore_meddra_embeddings_from_parts()
    embeddings = np.load("meddra_embeddings.npy")
    with open("meddra_terms.npy", "rb") as f:
        terms = np.load(f, allow_pickle=True)
    with open("term_master_df.pkl", "rb") as f:
        term_master_df = pickle.load(f)
    with open("synonym_df_cat1.pkl", "rb") as f:
        synonym_df = pickle.load(f)
    return terms, embeddings, term_master_df, synonym_df

@st.cache_resource
def load_faiss_index():
    index = faiss.read_index("faiss_index.index")
    return index

# ğŸ’» Streamlit UI
st.markdown("## ğŸ’Š MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")
st.write("ç—‡çŠ¶ã‚„è¨˜è¿°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

user_query = st.text_input("ç—‡çŠ¶å…¥åŠ›", "é ­ç—›")

if st.button("æ¤œç´¢"):
    if user_query:
        terms, embeddings, term_master_df, synonym_df = load_data()
        index = load_faiss_index()

        # âœ… ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_terms = expand_query_gpt(user_query)
        st.info(f"ğŸ” æ‹¡å¼µèª: {expanded_terms}")

        # âœ… FAISSæ¤œç´¢
        results = []
        for term in expanded_terms:
            query_vec = encode_query(term)
            D, I = index.search(np.array([query_vec]), k=10)
            for score, idx in zip(D[0], I[0]):
                if idx != -1:
                    row = {
                        "PT_Japanese": terms[idx]["PT_Japanese"],
                        "PT_English": terms[idx]["PT_English"],
                        "PT_ID": terms[idx]["PT_ID"],
                        "HLT_ID": terms[idx]["HLT_ID"],
                        "HLT_Japanese": terms[idx]["HLT_Japanese"],
                        "score": float(score),
                        "source": "FAISS"
                    }
                    results.append(row)
        faiss_df = pd.DataFrame(results)

        # âœ… ã‚·ãƒãƒ‹ãƒ æ¤œç´¢
        synonym_df_matched = match_synonyms(expanded_terms, synonym_df)

        # âœ… ãƒãƒ¼ã‚¸ãƒ»å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        merged_df = merge_faiss_and_synonym_results(faiss_df, synonym_df_matched)
        reranked = rerank_results_v13(merged_df)

        # âœ… çµæœè¡¨ç¤º
        st.write("### ğŸ” æ¤œç´¢çµæœ")
        st.dataframe(reranked)

        # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±è¡¨ç¤º
        if os.path.exists("score_cache.pkl"):
            st.success("âœ… score_cache.pklï¼ˆå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ã¯å­˜åœ¨ã—ã¾ã™")
        else:
            st.warning("âš ï¸ score_cache.pkl ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
