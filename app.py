import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import faiss
QUERY_CACHE_PATH = "data/query_expansion_cache.pkl"

from helper_functions import (
    encode_query,
    search_meddra,
    rerank_results_batch,
    add_hierarchy_info,
    rescale_scores,
    predict_soc_category,
    format_keywords,
    suggest_similar_terms, 
    load_score_cache, 
    load_query_cache, 
    add_hierarchy_info_jp,
    expand_query_gpt,
)

# ---------------- åˆæœŸè¨­å®š ---------------- #
st.set_page_config(page_title="MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª", page_icon="ğŸ”")
st.title("\U0001f50d MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")

# ---------------- ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---------------- #
@st.cache_resource
def load_assets():
    try:
        faiss_index = faiss.read_index("faiss_index.index")
    except Exception as e:
        st.error(f"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise e

    try:
        meddra_terms = np.load("meddra_terms.npy", allow_pickle=True)

        # âœ… synonym_df.pkl ã®ã¿èª­ã¿è¾¼ã¿
        synonym_path = "data/synonym_df.pkl"
        if not os.path.exists(synonym_path):
            st.error("âŒ synonym_df.pkl ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å…ˆã«ä½œæˆã—ã¦ãã ã•ã„ã€‚")
            raise FileNotFoundError("synonym_df.pkl not found")

        synonym_df = pickle.load(open(synonym_path, "rb"))

        # âœ… ã‚«ãƒ©ãƒ åãƒã‚§ãƒƒã‚¯
        if not {"variant", "PT_Japanese"}.issubset(synonym_df.columns):
            st.error("âŒ synonym_df ã«å¿…è¦ãªã‚«ãƒ©ãƒ ï¼ˆvariant / PT_Japaneseï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            raise ValueError("synonym_df ã®ã‚«ãƒ©ãƒ ä¸ä¸€è‡´")

        term_master_df = pickle.load(open("term_master_df.pkl", "rb"))

    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise e

    return faiss_index, meddra_terms, synonym_df, term_master_df




faiss_index, meddra_terms, synonym_df, term_master_df = load_assets()

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
score_cache = load_score_cache("score_cache.pkl")
query_cache = load_query_cache("query_expansion_cache.pkl")

# âœ… Streamlitã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
if st.sidebar.button("ğŸ—‘ï¸ ã‚¹ã‚³ã‚¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"):
    if os.path.exists("score_cache.pkl"):
        os.remove("score_cache.pkl")
        score_cache = {}
        st.sidebar.success("âœ… score_cache.pkl ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚å†å®Ÿè¡Œæ™‚ã«å†ä½œæˆã•ã‚Œã¾ã™ã€‚")
    else:
        st.sidebar.warning("âš ï¸ score_cache.pkl ã¯ã™ã§ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

# âœ… Streamlitã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
if st.sidebar.button("ğŸ—‘ï¸ æ‹¡å¼µèªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"):
    if os.path.exists(QUERY_CACHE_PATH):
        os.remove(QUERY_CACHE_PATH)
        query_cache = {}
        st.sidebar.success("æ‹¡å¼µèªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    else:
        st.sidebar.warning("æ‹¡å¼µèªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")


# ---------------- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ---------------- #
query = st.text_input("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šçš®è†šãŒã‹ã‚†ã„ï¼‰", value="ã‚ºã‚­ã‚ºã‚­")
# use_soc_filter = st.checkbox("GPTã«ã‚ˆã‚‹SOCäºˆæ¸¬ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰", value=True)
# âœ… 2025-04-14: GPTã«ã‚ˆã‚‹SOCãƒ•ã‚£ãƒ«ã‚¿ã¯å»ƒæ­¢

# ---------------- æ¤œç´¢å‡¦ç† ---------------- #
if st.button("æ¤œç´¢"):
    if not query.strip():
        st.warning("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        final_results = None
        with st.spinner("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è§£æä¸­..."):
            # âœ… STEP 1: ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆä¾‹ï¼šã€Œã‚ºã‚­ã‚ºã‚­ã€â†’ "headache", "migraine", ...ï¼‰
            predicted_keywords = expand_query_gpt(query, query_cache)

            # âœ… STEP 2: SOCã‚«ãƒ†ã‚´ãƒªäºˆæ¸¬ï¼ˆä¾‹ï¼šã€Œç¥çµŒç³»éšœå®³ã€ãªã©ã€ãƒ•ã‚£ãƒ«ã‚¿ç”¨ï¼‰
            soc_prediction = predict_soc_category(query)

            # âœ… STEP 3: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ã®è¡¨ç¤º
            # if query in query_cache:
            #     st.info("âœ… ã‚¯ã‚¨ãƒªæ‹¡å¼µã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚")
            # else:
            #     st.info("ğŸ†• æ–°ã—ã„æ‹¡å¼µèªã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è¿½åŠ æ¸ˆï¼‰ã€‚")

        # âœ… STEP 3.5: ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆæ‹¡å¼µèªã®ç¢ºèªï¼‰
        st.subheader("ğŸ§  GPTäºˆæ¸¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ•´å½¢å¾Œï¼‰")
        st.write(predicted_keywords)

        # âœ… STEP 4: FAISSæ¤œç´¢
        with st.spinner("FAISSã§ç”¨èªæ¤œç´¢ä¸­..."):
            search_results = []
            for kw in predicted_keywords:
                result = search_meddra(kw, faiss_index, meddra_terms, synonym_df, top_k=20)
                search_results.append(result)
            all_results = pd.concat(search_results).drop_duplicates(subset=["term"]).reset_index(drop=True)
            
        # âœ… STEP 5: GPTå†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        with st.spinner("å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ä¸­ï¼ˆGPTä¸€æ‹¬ï¼‰..."):
            score_cache = {}  # âœ… è¿½åŠ ï¼ˆAPIã‚³ãƒ¼ãƒ«ã‚’ç¹°ã‚Šè¿”ã•ãªã„ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
            reranked = rerank_results_batch(query, all_results, score_cache)
            reranked["score"] = rescale_scores(reranked["Relevance"].tolist())
            reranked["score"] = reranked["score"].map(lambda x: round(x, 1))  # å°æ•°1æ¡
            
        # âœ… STEP 5.5: LLT â†’ PT ã®è£œå®Œå‡¦ç†ï¼ˆterm â†’ PT_Japanese ã«æ­£è¦åŒ–ï¼‰
        try:
            # âœ… synonym_df ã«ã‚ˆã‚Š term ã¯ã™ã§ã« PT è¡¨è¨˜ã«ãªã£ã¦ã„ã‚‹å‰æã§ã‚³ãƒ”ãƒ¼
            reranked["term_mapped"] = reranked["term"]  # synonym_df ã«ã‚ˆã‚‹äº‹å‰è£œæ­£ã‚’ãã®ã¾ã¾æ¡ç”¨
    
            # âœ… ãƒ‡ãƒãƒƒã‚°ï¼šå¤‰æ›å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯èªä¸€è¦§ï¼ˆæŠœç²‹ï¼‰
            # mapped_terms = reranked["term_mapped"].unique().tolist()
            # st.write("ğŸ“Œ term_mappedï¼ˆå¤‰æ›å¾Œï¼‰æŠœç²‹:", mapped_terms[:10])

            # âœ… ãƒ‡ãƒãƒƒã‚°ï¼šPT_Japanese ã«ãƒãƒƒãƒã—ãªã‹ã£ãŸ term_mapped ã®ãƒã‚§ãƒƒã‚¯
            pt_set = set(term_master_df["PT_Japanese"].dropna())
            unmatched_pt = set(reranked["term_mapped"]) - pt_set
            # st.warning("ğŸ§¯ PT_Japanese ã«å­˜åœ¨ã—ãªã„ term_mappedï¼ˆä¸Šä½10ä»¶ï¼‰:")
            # st.write(list(unmatched_pt)[:10])

        except Exception as e:
            st.warning(f"LLTâ†’PTå¤‰æ›å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            reranked["term_mapped"] = reranked["term"]  # fallback ã‚’å¿…ãšä½œæˆ
            
        if final_results is None:
            final_results = reranked.copy()
            
            # âœ… ãƒ‡ãƒãƒƒã‚°ï¼šå¤‰æ›å¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯èªä¸€è¦§ï¼ˆæŠœç²‹ï¼‰
            # mapped_terms = reranked["term_mapped"].unique().tolist()
            # st.write("ğŸ“Œ term_mappedï¼ˆå¤‰æ›å¾Œï¼‰æŠœç²‹:", mapped_terms[:10])

            # âœ… ãƒ‡ãƒãƒƒã‚°ï¼šPT_Japanese ã«ãƒãƒƒãƒã—ãªã‹ã£ãŸ term_mapped ã®ãƒã‚§ãƒƒã‚¯
            pt_set = set(term_master_df["PT_Japanese"].dropna())
            unmatched_pt = set(reranked["term_mapped"]) - pt_set
            # st.warning("ğŸ§¯ PT_Japanese ã«å­˜åœ¨ã—ãªã„ term_mappedï¼ˆä¸Šä½10ä»¶ï¼‰:")
            # st.write(list(unmatched_pt)[:10])

            # STEP 5.6: matched_from åˆ—ã®è¿½åŠ 
            # synonym_df ã‚’ä½¿ã£ã¦è£œæ­£ã•ã‚ŒãŸ term ãŒã‚ã‚‹ã‹ç¢ºèª
            reranked["matched_from"] = "GPTæ‹¡å¼µèª"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

            # synonym_df ã«ã‚ã‚‹ variantï¼ˆ=å…ƒã®é¡ç¾©èªï¼‰ã‚’å…ƒã«è£œæ­£ã•ã‚ŒãŸèªãŒå«ã¾ã‚Œã‚‹ã‹åˆ¤å®š
            if "variant" in synonym_df.columns and "PT_Japanese" in synonym_df.columns:
                corrected_terms = synonym_df["PT_Japanese"].unique().tolist()
                reranked.loc[reranked["term"].isin(corrected_terms), "matched_from"] = "è¾æ›¸è£œæ­£"

            # âœ… STEP 6: MedDRAéšå±¤ä»˜åŠ 
            with st.spinner("éšå±¤æƒ…å ±ã‚’ä»˜åŠ ä¸­..."):

                # STEP 6.1: termåˆ—ã®æº–å‚™ï¼ˆterm_mapped â†’ term ã«ãƒªãƒãƒ¼ãƒ  or fallback ã§ç©ºåˆ—è¿½åŠ ï¼‰
                if "term_mapped" in reranked.columns:
                    df_for_merge = reranked.rename(columns={"term_mapped": "term"}).copy()
                elif "term" in reranked.columns:
                    df_for_merge = reranked.copy()
                else:
                    st.warning("âš ï¸ 'term' åˆ—ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ç©ºåˆ—ã‚’è¿½åŠ ã—ã¾ã™ã€‚")
                    df_for_merge = reranked.copy()
                    df_for_merge["term"] = ""

                # STEP 6.2: ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                try:
                    # st.write("ğŸ§ª df_for_merge ã®å‹:", type(df_for_merge))
                    # st.write("ğŸ§ª df_for_merge ã®ã‚«ãƒ©ãƒ :", df_for_merge.columns.tolist() if isinstance(df_for_merge, pd.DataFrame) else "ï¼ˆDataFrameã§ãªã„ï¼‰")

                    if isinstance(df_for_merge, pd.DataFrame) and "term" in df_for_merge.columns:
                        # preview = df_for_merge["term"].dropna().astype(str).unique().tolist()
                        # st.write("ğŸ§­ termåˆ—ï¼ˆéšå±¤ä»˜åŠ ç”¨ï¼‰ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ï¼ˆæŠœç²‹ï¼‰:", preview[:10])
                        pass  # è¡¨ç¤ºã ã‘OFF
                    else:
                        st.warning("âš ï¸ 'term' åˆ—ãŒ df_for_merge ã«å­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ df_for_merge ãŒ DataFrame ã§ãªã„å¯èƒ½æ€§ã‚ã‚Šã€‚")
                except Exception as e:
                    st.warning(f"âš ï¸ termåˆ—ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

                # STEP 6.3: éšå±¤æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ï¼ˆterm_mapped â†’ PT_Japaneseï¼‰
                try:
                    if "term_mapped" not in reranked.columns:
                        reranked["term_mapped"] = reranked["term"]

                    # âœ… term_master_dfã« "term" åˆ—ãŒã‚ã‚Œã°å‰Šé™¤ï¼ˆå¿µã®ãŸã‚ï¼‰
                    term_master_clean = term_master_df.drop(columns=["term"], errors="ignore")

                    final_results = pd.merge(
                        reranked,
                        term_master_clean,
                        how="left",
                        left_on="term_mapped",
                        right_on="PT_Japanese",
                        suffixes=("", "_master")
                    )

                    # âœ… é‡è¤‡ã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã€é™¤å»ï¼ˆStreamlitã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
                    if final_results.columns.duplicated().any():
                        final_results = final_results.loc[:, ~final_results.columns.duplicated()]

                    # st.write("ğŸ§© final_results ã®åˆ—ä¸€è¦§ï¼ˆç›´å¾Œï¼‰:", final_results.columns.tolist())
                except Exception as e:
                    st.error(f"âŒ éšå±¤ãƒã‚¹ã‚¿ã¨ã®ãƒãƒ¼ã‚¸ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    final_results = reranked.copy()


                # âœ… STEP 6.4: ãƒãƒ¼ã‚¸å¾Œã®ç¢ºèªã¨æœªä¸€è‡´ãƒã‚§ãƒƒã‚¯
                # st.write("ğŸ§© final_results ã®åˆ—ä¸€è¦§ï¼ˆSTEP 6.4ï¼‰:", final_results.columns.tolist())
                # st.write("ğŸ” ãƒãƒ¼ã‚¸å¯¾è±¡èªæ•°:", len(df_for_merge))
                # st.write("ğŸ” éšå±¤ä»˜ä¸å¾Œä»¶æ•°:", len(final_results))
                # st.write("ğŸ“‚ term_master_df ã®åˆ—ä¸€è¦§:", term_master_df.columns.tolist())
                
                base_terms = set(df_for_merge["term"]) if "term" in df_for_merge.columns else set()
                hier_terms = set(final_results["PT_Japanese"].dropna()) if "PT_Japanese" in final_results.columns else set()

                # unmatched_terms = base_terms - hier_terms
                # if unmatched_terms:
                    # st.warning("ğŸ§¯ éšå±¤ãƒã‚¹ã‚¿ã«ä¸€è‡´ã—ãªã‹ã£ãŸç”¨èªï¼ˆPT_Japaneseï¼‰:")
                    # st.write(list(unmatched_terms)[:10])

                
            # âœ… STEP 7: SOCãƒ•ã‚£ãƒ«ã‚¿ã¯å‰Šé™¤
            # ğŸ” æ¤œç´¢å®Œäº†ã®è¡¨ç¤ºã ã‘ã‚’æ®‹ã™
            st.success("æ¤œç´¢å®Œäº†")


            display_cols = [
                "term", "matched_from", "score",
                "PT_Japanese", "HLT_Japanese", "HLGT_Japanese", "SOC_Japanese"
            ]



            # STEP 8.0: å‹ã¨ä¸­èº«ãƒã‚§ãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦è¡Œã†
            if not isinstance(final_results, pd.DataFrame) or final_results.empty:
                st.error("âŒ final_results ãŒç©ºã€ã¾ãŸã¯DataFrameã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ¤œç´¢çµæœãŒå­˜åœ¨ã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                st.stop()
                
            # âœ… ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            # st.write("ğŸ” final_results ã®å‹:", type(final_results))
            # st.write("ğŸ” final_results ã®å…ˆé ­5è¡Œ:", final_results.head())       
                
            # STEP 8: è¡¨ç¤ºå¯¾è±¡ã‚«ãƒ©ãƒ ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            display_cols = [
                "term", "matched_from","score",
                "PT_Japanese", "HLT_Japanese", "HLGT_Japanese", "SOC_Japanese"
            ]
            available_cols = [col for col in display_cols if col in final_results.columns]

            # STEP 8.1: æ—¥æœ¬èªã«å¤‰æ›ã—ã¦è¡¨ç¤º
            st.dataframe(
                final_results[available_cols].rename(columns={
                    "term": "æ‹¡å¼µèª",
                    "matched_from": "ç”±æ¥",
                    "score": "ç¢ºã‹ã‚‰ã—ã• (%)",
                    "PT_Japanese": "PTï¼ˆæ—¥æœ¬èªï¼‰",
                    "HLT_Japanese": "HLTï¼ˆæ—¥æœ¬èªï¼‰",
                    "HLGT_Japanese": "HLGTï¼ˆæ—¥æœ¬èªï¼‰",
                    "SOC_Japanese": "SOCï¼ˆæ—¥æœ¬èªï¼‰"
                })
            )

        # CSVç”Ÿæˆæ™‚ã« encoding ã‚’æŒ‡å®šã™ã‚‹
        csv = final_results.to_csv(index=False, encoding="utf-8-sig")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        st.download_button("ğŸ“† çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="meddra_results.csv", mime="text/csv")
        
        # ğŸ” ãƒ†ã‚¹ãƒˆç”¨ãƒœã‚¿ãƒ³
        # if st.button("ğŸ” ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚ºã‚­ã‚ºã‚­ï¼‰"):
        #     from test_meddra_full_pipeline import run_test_pipeline
        #     run_test_pipeline()

        # âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜ï¼ˆæ¤œç´¢å®Œäº†å¾Œï¼‰
        with open("score_cache.pkl", "wb") as f:
            pickle.dump(score_cache, f)

        with open("query_expansion_cache.pkl", "wb") as f:
            pickle.dump(query_cache, f)

        # âœ… ã‚¹ãƒ†ãƒƒãƒ—Aï¼šæ„å‘³çš„ã«è¿‘ã„ç”¨èªå€™è£œã‚’è¡¨ç¤ºï¼ˆã‚ºã‚­ã‚ºã‚­ â†’ é ­ç—›ãªã©ï¼‰
        with st.expander("ğŸ§  é¡ä¼¼èªå€™è£œã‚’è¡¨ç¤ºï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼‰"):
            similar_terms = suggest_similar_terms(query, faiss_index, meddra_terms, top_k=10)
            st.write("ğŸ’¡ å…¥åŠ›èªã«æ„å‘³çš„ã«è¿‘ã„ç”¨èªå€™è£œ:")
            for i, term in enumerate(similar_terms, 1):
                st.markdown(f"{i}. {term}")

        import csv
        from datetime import datetime
        import os

        # STEP 9: æ¤œç´¢å±¥æ­´ã‚’ä¿å­˜ï¼ˆãƒ­ã‚°ã¨ã—ã¦ï¼‰
        log_path = "logs/search_history.csv"
        os.makedirs("logs", exist_ok=True)

        try:
            # st.write("ğŸ“‹ ãƒ­ã‚°ä¿å­˜å¯¾è±¡ä»¶æ•°:", len(final_results))  # â† ç¢ºèªç”¨
            pass  # è¡¨ç¤ºã¯ä¸€æ™‚OFF
            with open(log_path, mode="a", newline="", encoding="utf-8-sig") as f:
                writer = csv.writer(f)
                for _, row in final_results.iterrows():
                    writer.writerow([
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        query,
                        predicted_keywords,
                        row.get("term", ""),
                        row.get("matched_from", ""),
                        row.get("score", "")
                    ])
        except Exception as e:
            st.warning(f"âš ï¸ ãƒ­ã‚°ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            
        # âœ… STEP 10: æ¤œç´¢å±¥æ­´ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆãƒ­ã‚°ç”¨ï¼‰
        log_path = "logs/search_history.csv"
        if os.path.exists(log_path):
            with open(log_path, "r", encoding="utf-8-sig") as f:
                log_data = f.read()
            st.download_button(
                label="ğŸ“¥ æ¤œç´¢å±¥æ­´ãƒ­ã‚°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=log_data,
                file_name="search_history.csv",
                mime="text/csv"
            )
        else:
            st.info("â„¹ï¸ æ¤œç´¢å±¥æ­´ãƒ­ã‚°ã¯ã¾ã ä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")