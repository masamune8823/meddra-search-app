# app.pyï¼ˆã‚·ãƒãƒ‹ãƒ çµ±åˆï¼‹åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒå¯¾å¿œï¼‰

import streamlit as st
import pandas as pd
import numpy as np
import pickle
import faiss
import os
import zipfile

from helper_functions import (
    expand_query_gpt,
    encode_query,
    rerank_results_v13,
    match_synonyms,
    merge_faiss_and_synonym_results
)

# --- STEP 0: ãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒå‡¦ç† ---
with st.spinner("ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ä¸­..."):
    zip_path = "streamlit_app_bundle.zip"
    if os.path.exists(zip_path):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall()

# --- STEP 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
@st.cache_resource
def load_data():
    with open("term_master_df.pkl", "rb") as f:
        term_master_df = pickle.load(f)
    with open("synonym_df_cat1.pkl", "rb") as f:
        synonym_df = pickle.load(f)
    embeddings = np.load("meddra_embeddings.npy")
    return term_master_df, embeddings, synonym_df

# --- STEP 2: FAISS index ã®èª­ã¿è¾¼ã¿ ---
@st.cache_resource
def load_faiss_index():
    # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦å¾©å…ƒ
    with open("faiss_index_part_a", "rb") as f1, open("faiss_index_part_b", "rb") as f2:
        with open("faiss_index.index", "wb") as out_f:
            out_f.write(f1.read())
            out_f.write(f2.read())
    # å¾©å…ƒã—ãŸ index ã‚’èª­ã¿è¾¼ã¿
    index = faiss.read_index("faiss_index.index")
    return index

# --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
term_master_df, embeddings, synonym_df = load_data()
index = load_faiss_index()

# --- Streamlit UI ---
st.image("https://img.icons8.com/emoji/48/stethoscope-emoji.png", width=40)
st.markdown("## MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")
st.write("ç—‡çŠ¶ã‚„è¨˜è¿°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

query = st.text_input("ç—‡çŠ¶å…¥åŠ›", "")

if st.button("æ¤œç´¢"):
    if query.strip() == "":
        st.warning("ç—‡çŠ¶ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        # ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_terms = expand_query_gpt(query)

        # FAISSæ¤œç´¢
        results = []
        for term in expanded_terms:
            query_vec = encode_query(term)
            D, I = index.search(np.array([query_vec]), k=20)
            for dist, idx in zip(D[0], I[0]):
                pt = term_master_df.iloc[idx]["PT_Japanese"]
                results.append({"term": pt, "score": float(1 / (1 + dist))})  # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã«å¤‰æ›

        # ã‚¹ã‚³ã‚¢å†è¨ˆç®—ï¼ˆä»®ï¼‰
        reranked = rerank_results_v13(results)

        # ã‚·ãƒãƒ‹ãƒ è£œå®Œ
        matched = match_synonyms(query, synonym_df)

        # çµåˆï¼†è¡¨ç¤º
        final_results = merge_faiss_and_synonym_results(reranked, matched)

        st.markdown("### ğŸ” æ¤œç´¢çµæœ")
        if final_results:
            for res in final_results[:10]:
                st.write(f"â€¢ {res['term']}ï¼ˆç¢ºã‹ã‚‰ã—ã•: {res['score']:.2f}ï¼‰")
        else:
            st.info("ä¸€è‡´ã™ã‚‹ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
