
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦å‘¼ã³å‡ºã—ï¼‰
def load_score_cache(path="score_cache.pkl"):
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except:
        return {}

def load_query_cache(path="query_expansion_cache.pkl"):
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except:
        return {}

from openai import OpenAI
client = OpenAI()

import os
import re
import pickle
import openai
import faiss
import numpy as np
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer

# Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ Streamlit Secrets ã‹ã‚‰å–å¾—
hf_token = os.getenv("HF_TOKEN")

# ãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", use_auth_token=hf_token)

# OpenAI APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY")

# ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
def encode_query(text):
    return model.encode([text])[0]

# âœ… æ”¹è‰¯ç‰ˆ æ¤œç´¢å‡¦ç†ï¼ˆéƒ¨åˆ†ä¸€è‡´ + è¾æ›¸ + FAISSï¼‰v2
def search_meddra_v2(original_input, query, faiss_index, meddra_terms, synonym_df, top_k_faiss=10, matched_from_label=None):
    import pandas as pd

    results = []
    matched_terms = set()

    # âœ… 1. ã‚·ãƒãƒ‹ãƒ è¾æ›¸ï¼ˆvariant â†’ PT_Japaneseï¼‰
    if synonym_df is not None and "variant" in synonym_df.columns:
        synonym_hits = synonym_df[synonym_df["variant"] == query]
        for _, row in synonym_hits.iterrows():
            term = row["PT_Japanese"]
            if term not in matched_terms:
                results.append({"original_input": original_input,"term": term, "score": 1.0, "matched_from": "ã‚·ãƒãƒ‹ãƒ è¾æ›¸"})
                matched_terms.add(term)

    # âœ… 2. æ­£è¦è¾æ›¸ç…§åˆï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    for term in meddra_terms:
        if isinstance(term, str) and query.lower() in term.lower():
            if term not in matched_terms:
                results.append({ "original_input": original_input,"query": query,"term": term, "score": 1.0, "matched_from": "MedDRAè¾æ›¸ï¼ˆç”¨èªä¸€è‡´ï¼‰"})
                matched_terms.add(term)

    # âœ… 3. FAISSãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    from helper_functions import encode_query
    query_vector = encode_query(query).astype(np.float32)
    distances, indices = faiss_index.search(np.array([query_vector]), top_k_faiss)
    for i in range(len(indices[0])):
        idx = indices[0][i]
        if idx < len(meddra_terms):
            term_raw = meddra_terms[idx]
            term = term_raw.strip()

            results.append({
                "original_input": original_input,
                "query": query,
                "term": term,
                "score": float(distances[0][i]),
                "matched_from": matched_from_label or " MedDRAè¾æ›¸ï¼ˆé¡ä¼¼èªä¸€è‡´ï¼‰"
            })

    return pd.DataFrame(results)



#  ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼ã«ã‚ˆã‚‹PTå€™è£œæç¤º
def suggest_similar_terms(query, faiss_index, meddra_terms, top_k=10):
    """
    ã‚¯ã‚¨ãƒªã«æ„å‘³çš„ã«è¿‘ã„PTå€™è£œã‚’ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚Šå–å¾—ã™ã‚‹ã€‚
    """
    query_vector = encode_query(query).astype(np.float32)
    distances, indices = faiss_index.search(np.array([query_vector]), top_k)
    suggestions = [meddra_terms[idx] for idx in indices[0] if idx < len(meddra_terms)]
    return suggestions
   
# å›ç­”ã‹ã‚‰ã‚¹ã‚³ã‚¢æŠ½å‡ºï¼ˆå˜ç´”å®Ÿè£…ï¼‰
def extract_score_from_response(line):
    # ä¾‹: "1. 7" â†’ 7 ã‚’æŠ½å‡ºã€å…ˆé ­ã«æ•°å­—ã¨ãƒ”ãƒªã‚ªãƒ‰ãŒã‚ã‚‹å½¢å¼ã ã‘å¯¾è±¡
    import re
    match = re.match(r"^\d+\.\s*([0-9ï¼-ï¼™]+)", line.strip())
    if match:
        try:
            return float(match.group(1).replace("ï¼", "0").replace("ï¼‘", "1").replace("ï¼’", "2")
                                         .replace("ï¼“", "3").replace("ï¼”", "4").replace("ï¼•", "5")
                                         .replace("ï¼–", "6").replace("ï¼—", "7").replace("ï¼˜", "8")
                                         .replace("ï¼™", "9"))
        except:
            pass
    return 5.0  # fallback
    
    
# ã‚¹ã‚³ã‚¢ã®å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
def rescale_scores(scores):
    min_score = min(scores)
    max_score = max(scores)
    if max_score == min_score:
        return [100.0 for _ in scores]
    return [100.0 * (s - min_score) / (max_score - min_score) for s in scores]

# âœ… å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‡¦ç†ï¼ˆGPTä¸€æ‹¬å‘¼ã³å‡ºã—ç‰ˆï¼‰
# âœ… GPTå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‡¦ç†ï¼ˆ1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ï¼‰
def rerank_results_batch(original_input, candidates, score_cache=None):
    if score_cache is None:
        score_cache = {}

    top_candidates = candidates.head(10)
    
    # âœ… æ‹¡å¼µèªï¼ˆqueryï¼‰ã‚’ candidates ã‹ã‚‰å–å¾—ï¼ˆå¿…ãšã“ã“ã§å®šç¾©ï¼‰
    query = candidates["query"].iloc[0] if "query" in candidates.columns else ""
    
    # æœªã‚¹ã‚³ã‚¢ã® term ã ã‘ã‚’æŠ½å‡º
    new_terms = []
    for i, row in top_candidates.iterrows():
        term = row["term"]
        if (query, term) not in score_cache:
            new_terms.append(term)
    # âœ… ã‚¹ã‚³ã‚¢å¯¾è±¡ã®èªæ•°ã¨ä¸­èº«ã‚’Streamlitã§è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    import streamlit as st
    # st.write("ğŸ§ª ã‚¹ã‚³ã‚¢æœªè©•ä¾¡èªæ•°:", len(new_terms), "ä»¶")
    # st.write("ğŸ§ª æœªè©•ä¾¡èªãƒªã‚¹ãƒˆ:", new_terms)
    
    if new_terms:
        # ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦ï¼ˆ1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å…¨termï¼‰
        prompt = f"""ä»¥ä¸‹ã®è¨˜è¿°ã€Œ{original_input}ã€ã«å¯¾ã—ã¦ã€å„ç”¨èªãŒç—‡çŠ¶ã¾ãŸã¯ç–¾æ‚£ã®è¨˜è¿°å†…å®¹ã¨ã—ã¦ã©ã‚Œã ã‘æ„å‘³çš„ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
ä¸€è‡´åº¦ã‚’ 0ã€œ10 ã®æ•°å€¤ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

ã€Œä¸€è‡´ã€ã®åˆ¤æ–­ã¯ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š
- 10ï¼šå®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã‚‹ï¼ˆæ„å‘³ãŒåŒã˜ï¼‰
- 7ã€œ9ï¼šã‹ãªã‚Šè¿‘ã„ãŒå¾®å¦™ã«ç•°ãªã‚‹å ´åˆã‚‚ã‚ã‚‹
- 4ã€œ6ï¼šéƒ¨åˆ†çš„ã«é–¢é€£ã—ã¦ã„ã‚‹ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
- 1ã€œ3ï¼šã‚ãšã‹ã«é–¢é€£ã™ã‚‹ãŒã€åˆ¥ã®PTã®æ–¹ãŒæ˜ã‚‰ã‹ã«å¦¥å½“
- 0ï¼šã¾ã£ãŸãç„¡é–¢ä¿‚

"""
        for idx, term in enumerate(new_terms, 1):
            prompt += f"{idx}. {term}\n"

        prompt += "\nå½¢å¼ï¼š\n1. 7\n2. 5\n... ã®ã‚ˆã†ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"
        
        messages = [
            {"role": "system", "content": "ã‚ãªãŸã¯åŒ»è–¬å“å®‰å…¨ç›£è¦–æ¥­å‹™ã«ãŠã„ã¦ã€è‡ªç„¶è¨€èªã®ç—‡çŠ¶ãƒ»ç–¾æ‚£è¨˜è¿°ã¨MedDRAç”¨èªï¼ˆPTï¼‰ã¨ã®æ„å‘³çš„ä¸€è‡´åº¦ã‚’å°‚é–€çš„ã«è©•ä¾¡ã™ã‚‹æ‹…å½“è€…ã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0,
            )
            content = response.choices[0].message.content

            # âœ… Streamlitãƒ­ã‚°è¡¨ç¤ºï¼ˆå¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–ï¼‰
            import streamlit as st
            # st.subheader("ğŸ§¾ GPTãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆä¸€æ‹¬å½¢å¼ï¼‰")
            # st.code(content)

            for line in content.strip().split("\n"):
                if "." in line:
                    try:
                        idx_part, rest = line.split(".", 1)
                        score_str = rest.split(":")[-1].strip()  # ğŸ” ã€Œ:ã€ä»¥é™ã®ã‚¹ã‚³ã‚¢ã ã‘å–ã‚Šå‡ºã™
                        idx = int(idx_part.strip())
                        score = float(score_str)
                        term = new_terms[idx - 1]
                        score_cache[(original_input, term)] = score
                    except Exception as e:
                        st.warning(f"âŒ ã‚¹ã‚³ã‚¢æŠ½å‡ºå¤±æ•—: line='{line}' | error={e}")
                        continue
        except Exception as e:
            # ğŸ§ª ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: GPTã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã§ä¾‹å¤–ç™ºç”Ÿï¼ˆåˆå›ã®50%åŸå› èª¿æŸ»ç”¨ï¼‰
            st.warning(f"âŒ GPTã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã§ä¾‹å¤–ç™ºç”Ÿï¼ˆfallbackç™ºå‹•ï¼‰: {e}")
    
            for term in new_terms:
                score_cache[(original_input, term)] = 5.0  # fallback

    import streamlit as st

    # st.write("ğŸš€ rerank_results_batch() ã«åˆ°é”ã—ã¾ã—ãŸ")  # é–¢æ•°ãŒå‘¼ã°ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    
    # st.subheader("ğŸ§ª ä¿å­˜èªã¨è¡¨ç¤ºèªã®ä¸€è‡´ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥HITç¢ºèª")

    # new_terms ãƒ­ã‚°å‡ºåŠ›
    # st.text("ã€new_termsï¼ˆä¿å­˜å¯¾è±¡ï¼‰ã€‘")
    # st.write(new_terms)

    # top_candidates["term"] ãƒ­ã‚°å‡ºåŠ›
    # st.text("ã€top_candidates['term']ï¼ˆå–å¾—å¯¾è±¡ï¼‰ã€‘")
    # st.write(top_candidates["term"].tolist())

    # score_cache ãƒ­ã‚°å‡ºåŠ›
    # st.text("ã€score_cache ã®å†…å®¹ã€‘")
    # st.write(score_cache)

    # termã”ã¨ã«ä¸€è‡´ãƒ»HITãƒã‚§ãƒƒã‚¯
    for display_term in top_candidates["term"]:
        key = (original_input, display_term)
        hit = key in score_cache
        match_found = any(
            display_term.strip() == saved_term.strip()
            for saved_term in new_terms
        )
        # st.write(f"{key} â†’ {'âœ… HIT' if hit else 'âŒ MISS'} | termä¸€è‡´: {'âœ…' if match_found else 'âŒ'}")

    # new_terms ã®å†…å®¹ç¢ºèª
    # st.write("ğŸŸ¡ new_terms:", new_terms)

    # if new_terms:
        # st.success("âœ… new_terms ãŒå­˜åœ¨ã—ã€ã‚¹ã‚³ã‚¢è©•ä¾¡ãƒ–ãƒ­ãƒƒã‚¯ã«å…¥ã‚Šã¾ã—ãŸ")
    
    

    # ã‚¹ã‚³ã‚¢ã‚’ã¾ã¨ã‚ã¦è¿”ã™
    scored = [(term, score_cache.get((original_input, term), 5.0)) for term in top_candidates["term"]]
    df = pd.DataFrame(scored, columns=["term", "Relevance"])
    return df.sort_values(by="Relevance", ascending=False)





# GPTã§SOCã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬
def predict_soc_category(query):
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯MedDRAã®SOCã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": f"æ¬¡ã®ç—‡çŠ¶ã«æœ€ã‚‚é–¢é€£ã™ã‚‹MedDRAã®SOCã‚«ãƒ†ã‚´ãƒªã‚’1ã¤ã ã‘ã€æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œç¥çµŒç³»éšœå®³ã€ãªã©ï¼‰ã€‚\n\nç—‡çŠ¶: {query}"}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return "ä¸æ˜"

# ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆGPTä½¿ç”¨ï¼‰
def expand_query_gpt(query, query_cache=None):
    if query_cache is not None and query in query_cache:
        return query_cache[query]

    # GPTã¸å•ã„åˆã‚ã›ï¼ˆå…·ä½“çš„ãªç—‡çŠ¶åã‚’æ±‚ã‚ã‚‹ã‚ˆã†ã«æ”¹å–„ï¼‰
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã®ã‚ã„ã¾ã„ãªç—‡çŠ¶è¡¨ç¾ã‚’ã€æ­£ç¢ºãªè‹±èªã®åŒ»å­¦ç”¨èªã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": f"""
ä»¥ä¸‹ã®æ—¥æœ¬èªã®ç—‡çŠ¶ã€Œ{query}ã€ã«ã¤ã„ã¦ã€å…·ä½“çš„ã«è€ƒãˆã‚‰ã‚Œã‚‹åŒ»å­¦çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åï¼ˆè‹±èªï¼‰ã‚’3ã¤äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼šã€Œã‚ºã‚­ã‚ºã‚­ã€â†’ "headache", "migraine", "throbbing pain"

ãƒ»æ›–æ˜§ãªã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ï¼šç¥çµŒç³»éšœå®³ï¼‰ã§ã¯ãªãã€å…·ä½“çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ãƒ»å¿…ãš3ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""}
    ]

def expand_query_gpt(query, query_cache=None):
    if query_cache is not None and query in query_cache:
        return query_cache[query]

    # GPTã¸å•ã„åˆã‚ã›ï¼ˆå…·ä½“çš„ãªç—‡çŠ¶åã‚’æ±‚ã‚ã‚‹ã‚ˆã†ã«æ”¹å–„ï¼‰
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã®ã‚ã„ã¾ã„ãªç—‡çŠ¶è¡¨ç¾ã‚’ã€æ­£ç¢ºãªè‹±èªã®åŒ»å­¦ç”¨èªã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": f"""
ä»¥ä¸‹ã®æ—¥æœ¬èªã®ç—‡çŠ¶ã€Œ{query}ã€ã«ã¤ã„ã¦ã€å…·ä½“çš„ã«è€ƒãˆã‚‰ã‚Œã‚‹åŒ»å­¦çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åï¼ˆè‹±èªï¼‰ã‚’3ã¤äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼šã€Œã‚ºã‚­ã‚ºã‚­ã€â†’ "headache", "migraine", "throbbing pain"

ãƒ»æ›–æ˜§ãªã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ï¼šç¥çµŒç³»éšœå®³ï¼‰ã§ã¯ãªãã€å…·ä½“çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ãƒ»å¿…ãš3ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""}
    ]

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        response_text = response.choices[0].message.content.strip()

        # âœ… Streamlitãƒ­ã‚°ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºèª
        import streamlit as st
        # st.subheader("ğŸ“¥ GPT ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆæ‹¡å¼µèªï¼‰")
        # st.code(response_text)

        # ğŸ”§ æ•´å½¢å‡¦ç†ï¼ˆç•ªå· or ã‚«ãƒ³ãƒå¯¾å¿œï¼‰
        raw_lines = response_text.strip().split("\n")

        keywords = []
        for line in raw_lines:
            line = re.sub(r'^\d+\.\s*', '', line)
            line = line.strip().strip('"')
            if line:
                keywords.append(line)

        if not keywords:
            keywords = [kw.strip().strip('"') for kw in response_text.split(",") if kw.strip()]

        if query_cache is not None:
            query_cache[query] = keywords
        return keywords

    except Exception as e:
        return ["headache", "fever", "pain"]


# è¡¨ç¤ºæ•´å½¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼‰
def format_keywords(keywords):
    return "ã€".join(keywords)

# MedDRAéšå±¤æƒ…å ±ã‚’ä»˜ä¸
def add_hierarchy_info(df, term_master_df):
    merged = pd.merge(df, term_master_df, how="left", left_on="term", right_on="PT_English")
    return merged
    
# âœ… æ—¥æœ¬èªPTï¼ˆPT_Japaneseï¼‰ã§éšå±¤æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹é–¢æ•°
def add_hierarchy_info_jp(df, term_master_df):
    return pd.merge(df, term_master_df, how="left", left_on="term", right_on="PT_Japanese")
