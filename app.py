# app.pyï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆãƒ»åˆ†å‰²ZIPå¯¾å¿œï¼‰

import streamlit as st
import pandas as pd
import pickle
import numpy as np
import faiss
import os
import zipfile

from helper_functions import (
    expand_query_gpt,
    encode_query,
    rerank_results_v13,
    match_synonyms,
    merge_faiss_and_synonym_results
)

# åˆ†å‰²ZIPï¼ˆ.zip.001ï½ï¼‰ã‹ã‚‰1ã¤ã®ZIPã¸å¾©å…ƒã™ã‚‹é–¢æ•°
def restore_split_zip(base_name="streamlit_app_bundle.zip", part_count=4):
    if os.path.exists(base_name):
        return  # æ—¢ã«å¾©å…ƒæ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
    with open(base_name, "wb") as output:
        for i in range(part_count):
            part_name = f"{base_name}.{str(i+1).zfill(3)}"
            if not os.path.exists(part_name):
                raise FileNotFoundError(f"Missing part file: {part_name}")
            with open(part_name, "rb") as part_file:
                output.write(part_file.read())

# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ã™ã‚‹é–¢æ•°
def unzip_file(zip_path, extract_to="."):
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_to)

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿é–¢æ•°ï¼ˆZIPå¾©å…ƒè¾¼ã¿ï¼‰
@st.cache_resource
def load_data():
    # ZIPçµåˆï¼†å±•é–‹
    if not os.path.exists("faiss_index.index"):
        restore_split_zip()
        unzip_file("streamlit_app_bundle.zip")

    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªï¼ˆä¸‡ãŒä¸€ã®å±•é–‹å¤±æ•—å¯¾ç­–ï¼‰
    if not os.path.exists("faiss_index.index"):
        raise FileNotFoundError("faiss_index.index ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ZIPå±•é–‹ã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    if not os.path.exists("meddra_embeddings.npy"):
        raise FileNotFoundError("meddra_embeddings.npy ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    if not os.path.exists("term_master_df.pkl"):
        raise FileNotFoundError("term_master_df.pkl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    if not os.path.exists("synonym_df_cat1.pkl"):
        raise FileNotFoundError("synonym_df_cat1.pkl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    # èª­ã¿è¾¼ã¿å‡¦ç†
    index = faiss.read_index("faiss_index.index")
    embeddings = np.load("meddra_embeddings.npy")
    with open("term_master_df.pkl", "rb") as f:
        terms = pickle.load(f)
    with open("synonym_df_cat1.pkl", "rb") as f:
        synonym_df = pickle.load(f)

    return index, embeddings, terms, synonym_df

# æ¤œç´¢å®Ÿè¡Œé–¢æ•°
def search_terms(user_input, index, embeddings, terms, synonym_df):
    expanded_terms = expand_query_gpt(user_input)

    all_results = []
    for term in expanded_terms:
        query_vec = encode_query(term)
        D, I = index.search(np.array([query_vec]), k=10)
        for dist, idx in zip(D[0], I[0]):
            if idx < len(terms):
                row = terms.iloc[idx].to_dict()
                row["score"] = 100 - dist
                row["source"] = f"FAISS:{term}"
                all_results.append(row)

    faiss_df = pd.DataFrame(all_results)
    faiss_df = rerank_results_v13(faiss_df)
    synonym_df_filtered = match_synonyms(user_input, synonym_df)
    final_df = merge_faiss_and_synonym_results(faiss_df, synonym_df_filtered)

    return final_df

# UIéƒ¨åˆ†
st.markdown("## ğŸ’Š MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")
st.markdown("ç—‡çŠ¶ã‚„è¨˜è¿°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

user_input = st.text_input("ç—‡çŠ¶å…¥åŠ›", value="é ­ç—›")

if st.button("æ¤œç´¢"):
    with st.spinner("æ¤œç´¢ä¸­..."):
        try:
            index, embeddings, terms, synonym_df = load_data()
            results_df = search_terms(user_input, index, embeddings, terms, synonym_df)

            if not results_df.empty:
                st.success("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                st.dataframe(results_df)
            else:
                st.warning("è©²å½“ã™ã‚‹ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
