
import streamlit as st
import pandas as pd
import numpy as np
import os
import faiss
import pickle
import platform

from helper_functions import (
    encode_query,
    search_meddra,
    rerank_results_v13,
    add_hierarchy_info,
    rescale_scores,
)

# --- ç’°å¢ƒã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’åˆ‡ã‚Šæ›¿ãˆ ---
if "google" in platform.platform().lower():
    DATA_DIR = "/mnt/data"  # Google Colab
else:
    DATA_DIR = "."  # Streamlit Cloud

# --- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾© ---
index_path = os.path.join(DATA_DIR, "faiss_index.index")
terms_path = os.path.join(DATA_DIR, "meddra_terms.npy")
embed_path = os.path.join(DATA_DIR, "meddra_embeddings.npy")
synonym_path = os.path.join(DATA_DIR, "synonym_df_cat1.pkl")
hierarchy_path = os.path.join(DATA_DIR, "term_master_df.pkl")

# --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
try:
    with open(synonym_path, "rb") as f:
        synonym_df = pickle.load(f)
except Exception as e:
    st.warning(f"åŒç¾©èªè¾æ›¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    synonym_df = None

try:
    with open(hierarchy_path, "rb") as f:
        term_master_df = pickle.load(f)
except Exception as e:
    st.warning(f"term_master_df ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}")
    term_master_df = None

meddra_terms = np.load(terms_path, allow_pickle=True)
meddra_embeddings = np.load(embed_path)
faiss_index = faiss.read_index(index_path)

# --- Streamlit UI ---
st.title("ğŸ” MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")

query = st.text_input("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šçš®è†šãŒã‹ã‚†ã„ï¼‰", "")
use_filter = st.checkbox("GPTã«ã‚ˆã‚‹SOCäºˆæ¸¬ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰", value=False)

if st.button("æ¤œç´¢") and query:
    with st.spinner("æ¤œç´¢ä¸­..."):

        # ğŸ” æ¤œç´¢ï¼ˆsynonym_df + FAISSæ¤œç´¢ï¼‰
        results = search_meddra(query, faiss_index, meddra_terms, synonym_df, top_k=20)

        # ğŸ¯ Top10ä»¶ã‚’å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆGPTãƒ™ãƒ¼ã‚¹ï¼‰
        reranked = rerank_results_v13(query, results, top_n=10)

        # ğŸ§± éšå±¤æƒ…å ±ã®ä»˜åŠ ï¼ˆHLT/HLGT/SOCï¼‰
        final_results = add_hierarchy_info(reranked, term_master_df)

        # âœ… ğŸ“‹ ã¾ãšè¡¨ç¤ºç”¨ã®åˆ—åã«å¤‰æ›
        final_results = final_results.rename(columns={
            "term": "ç”¨èª",
            "score": "ç¢ºã‹ã‚‰ã—ã•ï¼ˆï¼…ï¼‰",
            "HLT_Japanese": "HLT",
            "HLGT_Japanese": "HLGT",
            "SOC_Japanese": "SOC"
        })

        # ğŸ“Š GPTã§é–¢é€£SOCã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬ã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if use_filter:
            try:
                from helper_functions import predict_soc_keywords_with_gpt
                predicted_keywords = predict_soc_keywords_with_gpt(query)
                final_results = final_results[
                    final_results[["SOC", "HLGT", "HLT"]].astype(str).apply(
                        lambda x: x.str.contains("|".join(predicted_keywords)).any(), axis=1
                    )
                ]
            except ImportError:
                st.warning("predict_soc_keywords_with_gpt é–¢æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")

        # ğŸ”¢ ã‚¹ã‚³ã‚¢å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆï¼…è¡¨ç¤ºï¼‰
        final_results = rescale_scores(final_results)

        # âœ… è¡¨ç¤ºç”¨ã‚«ãƒ©ãƒ ã®æœ€çµ‚æ•´å½¢
        final_results = final_results[["ç”¨èª", "ç¢ºã‹ã‚‰ã—ã•ï¼ˆï¼…ï¼‰", "HLT", "HLGT", "SOC"]]

        st.success("æ¤œç´¢å®Œäº†")
        st.dataframe(final_results, use_container_width=True)
