# app.pyï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆãƒ»åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œãƒ»ãƒ‡ã‚°ãƒ¬ãªã—ï¼‰
import streamlit as st
import pandas as pd
import numpy as np
import pickle
import faiss
import os

from helper_functions import (
    expand_query_gpt,
    encode_query,
    rerank_results_v13,
    match_synonyms,
    merge_faiss_and_synonym_results
)

# ğŸ”§ FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¾©å…ƒé–¢æ•°
def restore_faiss_index_from_parts():
    part_a = "faiss_index_part_a"
    part_b = "faiss_index_part_b"
    output = "faiss_index.index"
    if not os.path.exists(output):
        with open(output, "wb") as f_out:
            for part in [part_a, part_b]:
                with open(part, "rb") as f_in:
                    f_out.write(f_in.read())

# ğŸ”§ ãƒ™ã‚¯ãƒˆãƒ«å¾©å…ƒé–¢æ•°
def restore_meddra_embeddings_from_parts():
    part_a = "meddra_embeddings_part_a"
    part_b = "meddra_embeddings_part_b"
    output = "meddra_embeddings.npy"
    if not os.path.exists(output):
        with open(output, "wb") as f_out:
            for part in [part_a, part_b]:
                with open(part, "rb") as f_in:
                    f_out.write(f_in.read())

# ğŸ” åˆå›ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def load_data():
    restore_faiss_index_from_parts()
    restore_meddra_embeddings_from_parts()

    # ãƒ™ã‚¯ãƒˆãƒ«ã¨ç”¨èªãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
    embeddings = np.load("meddra_embeddings.npy")
    with open("meddra_terms.npy", "rb") as f:
        terms = np.load(f, allow_pickle=True)

    # ãƒã‚¹ã‚¿ã¨ã‚·ãƒãƒ‹ãƒ è¾æ›¸ã®èª­ã¿è¾¼ã¿
    with open("term_master_df.pkl", "rb") as f:
        term_master_df = pickle.load(f)
    with open("synonym_df_cat1.pkl", "rb") as f:
        synonym_df = pickle.load(f)

    return terms, embeddings, term_master_df, synonym_df

# ğŸ” FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿
@st.cache_resource
def load_faiss_index():
    index = faiss.read_index("faiss_index.index")
    return index

# ğŸ’» UIæœ¬ä½“
st.markdown("## ğŸ’Š MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")
st.write("ç—‡çŠ¶ã‚„è¨˜è¿°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

user_query = st.text_input("ç—‡çŠ¶å…¥åŠ›", "é ­ç—›")

if st.button("æ¤œç´¢"):
    if user_query:
        try:
            terms, embeddings, term_master_df, synonym_df = load_data()
            index = load_faiss_index()

            # ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆOpenAI API ã¾ãŸã¯ä»®ã®å‡¦ç†ï¼‰
            expanded_terms = expand_query_gpt(user_query)

            # æ¤œç´¢å‡¦ç†
            results = []
            for term in expanded_terms:
                query_vec = encode_query(term)
                D, I = index.search(np.array([query_vec]), k=10)
                for score, idx in zip(D[0], I[0]):
                    results.append({"term": terms[idx], "score": float(score)})

            # ã‚·ãƒãƒ‹ãƒ æ¤œç´¢
            synonym_matches = match_synonyms(expanded_terms, synonym_df)

            # ãƒãƒ¼ã‚¸ã—ã¦å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            merged = merge_faiss_and_synonym_results(results, synonym_matches)
            reranked = rerank_results_v13(merged)

            # çµæœè¡¨ç¤º
            df = pd.DataFrame(reranked)
            st.write("### ğŸ” æ¤œç´¢çµæœï¼ˆä¸Šä½ï¼‰")
            st.dataframe(df)

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
