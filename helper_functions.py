
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦å‘¼ã³å‡ºã—ï¼‰
def load_score_cache(path="score_cache.pkl"):
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except:
        return {}

def load_query_cache(path="query_expansion_cache.pkl"):
    try:
        with open(path, "rb") as f:
            return pickle.load(f)
    except:
        return {}

from openai import OpenAI
client = OpenAI()

import os
import re
import pickle
import openai
import faiss
import numpy as np
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer

# Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ Streamlit Secrets ã‹ã‚‰å–å¾—
hf_token = os.getenv("HF_TOKEN")

# ãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", use_auth_token=hf_token)

# OpenAI APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY")

# ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
def encode_query(text):
    return model.encode([text])[0]

# âœ… æ”¹è‰¯ç‰ˆ æ¤œç´¢å‡¦ç†ï¼ˆéƒ¨åˆ†ä¸€è‡´ + è¾æ›¸ + FAISSï¼‰v2
def search_meddra_v2(query, faiss_index, meddra_terms, synonym_df, top_k_faiss=10, matched_from_label=None):
    import pandas as pd

    results = []
    matched_terms = set()

    # âœ… 1. ã‚·ãƒãƒ‹ãƒ è¾æ›¸ï¼ˆvariant â†’ PT_Japaneseï¼‰
    if synonym_df is not None and "variant" in synonym_df.columns:
        synonym_hits = synonym_df[synonym_df["variant"] == query]
        for _, row in synonym_hits.iterrows():
            term = row["PT_Japanese"]
            if term not in matched_terms:
                results.append({"term": term, "score": 1.0, "matched_from": "ã‚·ãƒãƒ‹ãƒ è¾æ›¸æ¤œç´¢"})
                matched_terms.add(term)

    # âœ… 2. æ­£è¦è¾æ›¸ç…§åˆï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    for term in meddra_terms:
        if isinstance(term, str) and query.lower() in term.lower():
            if term not in matched_terms:
                results.append({"term": term, "score": 1.0, "matched_from": "æ­£è¦è¾æ›¸ç…§åˆæ¤œç´¢"})
                matched_terms.add(term)

    # âœ… 3. FAISSãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    from helper_functions import encode_query
    query_vector = encode_query(query).astype(np.float32)
    distances, indices = faiss_index.search(np.array([query_vector]), top_k_faiss)
    for i in range(len(indices[0])):
        idx = indices[0][i]
        if idx < len(meddra_terms):
            term_raw = meddra_terms[idx]
            # âœ… "English / æ—¥æœ¬èª" ã®å½¢å¼ãªã‚‰æ—¥æœ¬èªã ã‘ä½¿ã†
            term = term_raw.split("/")[-1].strip() if "/" in term_raw else term_raw.strip()

            results.append({
                "term": term,
                "score": float(distances[0][i]),
                "matched_from": matched_from_label or " FAISSãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
            })

    return pd.DataFrame(results)



#  ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼ã«ã‚ˆã‚‹PTå€™è£œæç¤º
def suggest_similar_terms(query, faiss_index, meddra_terms, top_k=10):
    """
    ã‚¯ã‚¨ãƒªã«æ„å‘³çš„ã«è¿‘ã„PTå€™è£œã‚’ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚Šå–å¾—ã™ã‚‹ã€‚
    """
    query_vector = encode_query(query).astype(np.float32)
    distances, indices = faiss_index.search(np.array([query_vector]), top_k)
    suggestions = [meddra_terms[idx] for idx in indices[0] if idx < len(meddra_terms)]
    return suggestions
   
# å›ç­”ã‹ã‚‰ã‚¹ã‚³ã‚¢æŠ½å‡ºï¼ˆå˜ç´”å®Ÿè£…ï¼‰
def extract_score_from_response(response_text):
    for word in ["10", "ï¼™", "8", "ï¼—", "6", "5", "4", "3", "2", "1", "0"]:
        if word in response_text:
            try:
                return float(word)
            except:
                continue
    return 5.0  # fallback

# ã‚¹ã‚³ã‚¢ã®å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
def rescale_scores(scores):
    min_score = min(scores)
    max_score = max(scores)
    if max_score == min_score:
        return [100.0 for _ in scores]
    return [100.0 * (s - min_score) / (max_score - min_score) for s in scores]

# âœ… å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‡¦ç†ï¼ˆGPTä¸€æ‹¬å‘¼ã³å‡ºã—ç‰ˆï¼‰
# âœ… GPTå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‡¦ç†ï¼ˆ1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ï¼‰
def rerank_results_batch(query, candidates, score_cache=None):
    if score_cache is None:
        score_cache = {}

    top_candidates = candidates.head(10)

    # æœªã‚¹ã‚³ã‚¢ã® term ã ã‘ã‚’æŠ½å‡º
    new_terms = []
    for i, row in top_candidates.iterrows():
        term = row["term"]
        if (query, term) not in score_cache:
            new_terms.append(term)
    # âœ… ã‚¹ã‚³ã‚¢å¯¾è±¡ã®èªæ•°ã¨ä¸­èº«ã‚’Streamlitã§è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    import streamlit as st
    # st.write("ğŸ§ª ã‚¹ã‚³ã‚¢æœªè©•ä¾¡èªæ•°:", len(new_terms), "ä»¶")
    # st.write("ğŸ§ª æœªè©•ä¾¡èªãƒªã‚¹ãƒˆ:", new_terms)
    
    if new_terms:
        # ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦ï¼ˆ1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å…¨termï¼‰
        prompt = f"""ä»¥ä¸‹ã®è¨˜è¿°ã€Œ{query}ã€ã«å¯¾ã—ã¦ã€å„ç”¨èªãŒã©ã‚Œãã‚‰ã„æ„å‘³çš„ã«ä¸€è‡´ã™ã‚‹ã‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚
ä¸€è‡´åº¦ã‚’ 0ã€œ10 ã®æ•°å€¤ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

"""
        for idx, term in enumerate(new_terms, 1):
            prompt += f"{idx}. {term}\n"

        prompt += "\nå½¢å¼ï¼š\n1. 7\n2. 5\n... ã®ã‚ˆã†ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚"

        messages = [
            {"role": "system", "content": "ã‚ãªãŸã¯åŒ»ç™‚ç”¨èªã®é–¢é€£æ€§ã‚’æ•°å€¤ã§åˆ¤æ–­ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0,
            )
            content = response.choices[0].message.content

            # âœ… Streamlitãƒ­ã‚°è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            import streamlit as st
            # st.subheader("ğŸ§¾ GPTãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆä¸€æ‹¬å½¢å¼ï¼‰")
            #  st.code(content)

            # æ•°å€¤æŠ½å‡ºï¼ˆå½¢å¼ï¼š1. 7ï¼‰
            for line in content.strip().split("\n"):
                if "." in line:
                    parts = line.split(".")
                    try:
                        idx = int(parts[0].strip())
                        score = extract_score_from_response(line)
                        term = new_terms[idx - 1]
                        score_cache[(query, term)] = score
                    except:
                        continue
        except Exception as e:
            for term in new_terms:
                score_cache[(query, term)] = 5.0  # fallback

    # ã‚¹ã‚³ã‚¢ã‚’ã¾ã¨ã‚ã¦è¿”ã™
    scored = [(term, score_cache.get((query, term), 5.0)) for term in top_candidates["term"]]
    df = pd.DataFrame(scored, columns=["term", "Relevance"])
    return df.sort_values(by="Relevance", ascending=False)


# GPTã§SOCã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬
def predict_soc_category(query):
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯MedDRAã®SOCã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": f"æ¬¡ã®ç—‡çŠ¶ã«æœ€ã‚‚é–¢é€£ã™ã‚‹MedDRAã®SOCã‚«ãƒ†ã‚´ãƒªã‚’1ã¤ã ã‘ã€æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œç¥çµŒç³»éšœå®³ã€ãªã©ï¼‰ã€‚\n\nç—‡çŠ¶: {query}"}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return "ä¸æ˜"

# ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆGPTä½¿ç”¨ï¼‰
def expand_query_gpt(query, query_cache=None):
    if query_cache is not None and query in query_cache:
        return query_cache[query]

    # GPTã¸å•ã„åˆã‚ã›ï¼ˆå…·ä½“çš„ãªç—‡çŠ¶åã‚’æ±‚ã‚ã‚‹ã‚ˆã†ã«æ”¹å–„ï¼‰
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã®ã‚ã„ã¾ã„ãªç—‡çŠ¶è¡¨ç¾ã‚’ã€æ­£ç¢ºãªè‹±èªã®åŒ»å­¦ç”¨èªã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": f"""
ä»¥ä¸‹ã®æ—¥æœ¬èªã®ç—‡çŠ¶ã€Œ{query}ã€ã«ã¤ã„ã¦ã€å…·ä½“çš„ã«è€ƒãˆã‚‰ã‚Œã‚‹åŒ»å­¦çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åï¼ˆè‹±èªï¼‰ã‚’3ã¤äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼šã€Œã‚ºã‚­ã‚ºã‚­ã€â†’ "headache", "migraine", "throbbing pain"

ãƒ»æ›–æ˜§ãªã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ï¼šç¥çµŒç³»éšœå®³ï¼‰ã§ã¯ãªãã€å…·ä½“çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ãƒ»å¿…ãš3ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""}
    ]

def expand_query_gpt(query, query_cache=None):
    if query_cache is not None and query in query_cache:
        return query_cache[query]

    # GPTã¸å•ã„åˆã‚ã›ï¼ˆå…·ä½“çš„ãªç—‡çŠ¶åã‚’æ±‚ã‚ã‚‹ã‚ˆã†ã«æ”¹å–„ï¼‰
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã®ã‚ã„ã¾ã„ãªç—‡çŠ¶è¡¨ç¾ã‚’ã€æ­£ç¢ºãªè‹±èªã®åŒ»å­¦ç”¨èªã«å¤‰æ›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": f"""
ä»¥ä¸‹ã®æ—¥æœ¬èªã®ç—‡çŠ¶ã€Œ{query}ã€ã«ã¤ã„ã¦ã€å…·ä½“çš„ã«è€ƒãˆã‚‰ã‚Œã‚‹åŒ»å­¦çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åï¼ˆè‹±èªï¼‰ã‚’3ã¤äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

ä¾‹ï¼šã€Œã‚ºã‚­ã‚ºã‚­ã€â†’ "headache", "migraine", "throbbing pain"

ãƒ»æ›–æ˜§ãªã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ï¼šç¥çµŒç³»éšœå®³ï¼‰ã§ã¯ãªãã€å…·ä½“çš„ãªç—‡çŠ¶åã‚„ç–¾æ‚£åã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ãƒ»å¿…ãš3ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""}
    ]

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        response_text = response.choices[0].message.content.strip()

        # âœ… Streamlitãƒ­ã‚°ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºèª
        import streamlit as st
        # st.subheader("ğŸ“¥ GPT ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆæ‹¡å¼µèªï¼‰")
        # st.code(response_text)

        # ğŸ”§ æ•´å½¢å‡¦ç†ï¼ˆç•ªå· or ã‚«ãƒ³ãƒå¯¾å¿œï¼‰
        raw_lines = response_text.strip().split("\n")

        keywords = []
        for line in raw_lines:
            line = re.sub(r'^\d+\.\s*', '', line)
            line = line.strip().strip('"')
            if line:
                keywords.append(line)

        if not keywords:
            keywords = [kw.strip().strip('"') for kw in response_text.split(",") if kw.strip()]

        if query_cache is not None:
            query_cache[query] = keywords
        return keywords

    except Exception as e:
        return ["headache", "fever", "pain"]


# è¡¨ç¤ºæ•´å½¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼‰
def format_keywords(keywords):
    return "ã€".join(keywords)

# MedDRAéšå±¤æƒ…å ±ã‚’ä»˜ä¸
def add_hierarchy_info(df, term_master_df):
    merged = pd.merge(df, term_master_df, how="left", left_on="term", right_on="PT_English")
    return merged
    
# âœ… æ—¥æœ¬èªPTï¼ˆPT_Japaneseï¼‰ã§éšå±¤æƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹é–¢æ•°
def add_hierarchy_info_jp(df, term_master_df):
    return pd.merge(df, term_master_df, how="left", left_on="term", right_on="PT_Japanese")
