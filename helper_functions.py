# âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«ä¸€åº¦ã ã‘æ›¸ã
from openai import OpenAI
client = OpenAI()

import os
import re
import pickle
import openai
import faiss
import numpy as np
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# OpenAI APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY")

# ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
def encode_query(text):
    return model.encode([text])[0]

# æ¤œç´¢å‡¦ç†æœ¬ä½“
def search_meddra(query, faiss_index, meddra_terms, synonym_df=None, top_k=10):
    query_vector = encode_query(query).astype(np.float32)
    distances, indices = faiss_index.search(np.array([query_vector]), top_k)
    results = []
    for i in range(len(indices[0])):
        idx = indices[0][i]
        if idx < len(meddra_terms):
            term = meddra_terms[idx]
            score = float(distances[0][i])
            results.append({"term": term, "score": score})
    return pd.DataFrame(results)

# å›ç­”ã‹ã‚‰ã‚¹ã‚³ã‚¢æŠ½å‡ºï¼ˆå˜ç´”å®Ÿè£…ï¼‰
def extract_score_from_response(response_text):
    for word in ["10", "ï¼™", "8", "ï¼—", "6", "5", "4", "3", "2", "1", "0"]:
        if word in response_text:
            try:
                return float(word)
            except:
                continue
    return 5.0  # fallback

# ã‚¹ã‚³ã‚¢ã®å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
def rescale_scores(scores):
    min_score = min(scores)
    max_score = max(scores)
    if max_score == min_score:
        return [100.0 for _ in scores]
    return [100.0 * (s - min_score) / (max_score - min_score) for s in scores]

# âœ… å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‡¦ç†ï¼ˆGPTä¸€æ‹¬å‘¼ã³å‡ºã—ç‰ˆï¼‰
def rerank_results_batch(query, candidates, score_cache=None):
    if score_cache is None:
        score_cache = {}

    # Top10ä»¶ã«çµã‚‹
    top_candidates = candidates.head(10)

    messages = [{"role": "system", "content": "ã‚ãªãŸã¯åŒ»ç™‚ç”¨èªã®é–¢é€£æ€§åˆ¤å®šãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"}]
    index_map = {}  # idxã¨termã®å¯¾å¿œã‚’è¨˜éŒ²

    for i, row in top_candidates.iterrows():
        term = row["term"]
        cache_key = (query, term)

        if cache_key in score_cache:
            continue  # ã‚¹ã‚³ã‚¢æ¸ˆã¿

        prompt = f"ç”¨èªã€Œ{term}ã€ã¯ã€ä»¥ä¸‹ã®è¨˜è¿°ã¨ã©ã‚Œãã‚‰ã„æ„å‘³çš„ã«ä¸€è‡´ã—ã¾ã™ã‹ï¼Ÿ ä¸€è‡´åº¦ï¼ˆ0ï½10ï¼‰ã‚’æ•°å€¤ã§æ•™ãˆã¦ãã ã•ã„ã€‚\nè¨˜è¿°: {query}"
        messages.append({"role": "user", "content": prompt})
        index_map[len(messages) - 2] = term  # systemã‚’é™¤ã„ãŸindex

    # GPTå‘¼ã³å‡ºã—ï¼ˆ1å›ï¼‰
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        
        # âœ… ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°ï¼ˆå‰å¾Œã‚’å«ã‚ã¦æ˜ç¤ºï¼‰
        import streamlit as st
        st.subheader("ğŸ§¾ GPTãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰")
        st.write("ğŸ” GPTã‹ã‚‰ã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå…¨ä½“æ§‹é€ ï¼‰")
        st.write(response)  # â† responseå…¨ä½“ã‚’è¡¨ç¤ºï¼ˆæ§‹é€ ç¢ºèªï¼‰

        st.write("ğŸ“ GPTã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ¬æ–‡ï¼ˆchoices[0].message.contentï¼‰")
        st.code(response.choices[0].message.content)  # â† å®Ÿéš›ã®è¿”ç­”ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º

        # è¿”ç­”ï¼ˆ1ã¤ï¼‰ã‹ã‚‰å…¨ä½“ã®å†…å®¹ã‚’å–å¾—
        content = response.choices[0].message.content

        # è¿”ç­”ã®ä¸­ã‹ã‚‰å€‹åˆ¥ã«æ•°å€¤ã‚’æŠ½å‡ºï¼ˆæ”¹è¡Œ or ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæƒ³å®šï¼‰
        lines = [l.strip() for l in content.strip().split("\n") if l.strip()]
        for i, line in enumerate(lines):
            st.write(f"[{i}] {line}")  # â† ğŸ” ã©ã‚“ãªè¡Œã‹ã‚’å¯è¦–åŒ–
            if i in index_map:
                term = index_map[i]
                try:
                    score = extract_score_from_response(line)
                    score_cache[(query, term)] = score
                except:
                    score_cache[(query, term)] = 5.0  # fallback
    except Exception as e:
        # å…¨ä½“å¤±æ•—æ™‚ã®fallback
        for term in top_candidates["term"]:
            score_cache[(query, term)] = 5.0

    # ã‚¹ã‚³ã‚¢ã‚’ã¾ã¨ã‚ã¦è¿”ã™
    scored = [(term, score_cache.get((query, term), 5.0)) for term in top_candidates["term"]]
    df = pd.DataFrame(scored, columns=["term", "Relevance"])
    return df.sort_values(by="Relevance", ascending=False)

# GPTã§SOCã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬
def predict_soc_category(query):
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯åŒ»ç™‚åˆ†é‡ã«è©³ã—ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": f"æ¬¡ã®ç—‡çŠ¶ã«æœ€ã‚‚é–¢é€£ã™ã‚‹MedDRAã®SOCã‚«ãƒ†ã‚´ãƒªã‚’æ•™ãˆã¦ãã ã•ã„:ã€Œ{query}ã€"}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        return response.choices[0].message.content
    except Exception as e:
        return "ã‚¨ãƒ©ãƒ¼: " + str(e)

# ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆGPTä½¿ç”¨ï¼‰
def expand_query_gpt(query):
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªåŒ»ç™‚æ–‡ã‚’è‹±èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": f"ä»¥ä¸‹ã®æ—¥æœ¬èªã®ç—‡çŠ¶ã‹ã‚‰ã€è‹±èªã®åŒ»å­¦çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚\n\nç—‡çŠ¶: {query}"}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0,
        )
        response_text = response.choices[0].message.content
        return [kw.strip() for kw in response_text.split(",") if kw.strip()]
    except Exception as e:
        return ["headache", "nausea", "fever"]

# è¡¨ç¤ºæ•´å½¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼‰
def format_keywords(keywords):
    return "ã€".join(keywords)

# MedDRAéšå±¤æƒ…å ±ã‚’ä»˜ä¸
def add_hierarchy_info(df, term_master_df):
    merged = pd.merge(df, term_master_df, how="left", left_on="term", right_on="PT_English")
    return merged
# update 