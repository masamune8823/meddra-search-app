import os
import zipfile
import streamlit as st
import pandas as pd
import numpy as np
import pickle
import faiss
from utils import expand_query_gpt, encode_query, rerank_results_v13

# æ±ç”¨ï¼šåˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã®çµåˆ
def restore_split_file(output_path, parts, folder="."):
    with open(output_path, "wb") as outfile:
        for part in parts:
            part_path = os.path.join(folder, f"{output_path}_part_{part}")
            if os.path.exists(part_path):
                with open(part_path, "rb") as infile:
                    outfile.write(infile.read())
            else:
                raise FileNotFoundError(f"{part_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# 1. search_assets.zip ã®å¾©å…ƒã¨å±•é–‹
def restore_search_assets():
    zip_name = "search_assets"
    parts = ["a", "b", "c", "d"]
    restore_split_file(zip_name, parts, folder=".")
    with zipfile.ZipFile(f"{zip_name}.zip", 'r') as zip_ref:
        zip_ref.extractall("data")

# 2. meddra_embeddings.npy ã®å¾©å…ƒï¼ˆè§£å‡ä¸è¦ï¼‰
def restore_embeddings():
    output_path = "meddra_embeddings.npy"
    parts = ["a", "b"]
    restore_split_file(output_path, parts, folder=".")

# 3. faiss_index.index ã®å¾©å…ƒï¼ˆzipå±•é–‹ï¼‰
def restore_faiss_index_zip():
    zip_name = "faiss_index"
    parts = ["a", "b"]
    restore_split_file(zip_name, parts, folder=".")
    with zipfile.ZipFile(f"{zip_name}.zip", 'r') as zip_ref:
        zip_ref.extractall("data")

# âœ… ãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒã®å®Ÿè¡Œ
restore_search_assets()
restore_embeddings()
restore_faiss_index_zip()

# =============================
# Streamlit UI æœ¬ä½“
# =============================

@st.cache_resource
def load_faiss_and_data():
    index = faiss.read_index("data/faiss_index.index")
    embeddings = np.load("data/meddra_embeddings.npy")
    with open("data/meddra_terms.npy", "rb") as f:
        terms = np.load(f, allow_pickle=True)
    with open("data/term_master_df.pkl", "rb") as f:
        master_df = pickle.load(f)
    return index, terms, master_df

faiss_index, meddra_terms, term_master_df = load_faiss_and_data()

st.set_page_config(page_title="MedDRAæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")
st.title("ğŸ©º MedDRAæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—UIï¼‰")

st.header("1. åŒ»å¸«è¨˜è¼‰ç”¨èªã®å…¥åŠ›")
user_input = st.text_area("è‡ªç„¶è¨€èªã§è¨˜è¼‰ã•ã‚ŒãŸç—‡çŠ¶ã‚„å‡ºæ¥äº‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", height=100)

if st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ") and user_input:
    st.header("2. ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼‰")
    expanded_terms = expand_query_gpt(user_input)
    st.write("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š", expanded_terms)

    all_results = []
    for term in expanded_terms:
        query_vec = encode_query(term)
        scores, indices = faiss_index.search(np.array([query_vec]), k=20)
        for score, idx in zip(scores[0], indices[0]):
            result = {
                "term": meddra_terms[idx],
                "score": float(score),
                "query_term": term
            }
            all_results.append(result)

    reranked = rerank_results_v13(user_input, all_results)
    results_df = pd.DataFrame(reranked)
    merged_df = pd.merge(results_df, term_master_df, how="left", left_on="term", right_on="PT_English")
    merged_df = merged_df[["score", "term", "PT_Japanese", "HLT_Japanese", "HLGT_Japanese", "SOC_Japanese"]].copy()
    merged_df["ç¢ºã‹ã‚‰ã—ã•"] = (merged_df["score"] * 10).clip(0, 100).round(1).astype(str) + "%"
    merged_df.rename(columns={"term": "PT_English"}, inplace=True)
    merged_df = merged_df[["ç¢ºã‹ã‚‰ã—ã•", "PT_Japanese", "PT_English", "HLT_Japanese", "HLGT_Japanese", "SOC_Japanese"]]

    st.header("3. æ¤œç´¢çµæœï¼ˆTop10å€™è£œï¼‹ç¢ºã‹ã‚‰ã—ã•ï¼‰")
    st.dataframe(merged_df.head(10), use_container_width=True)

    st.header("4. å‡ºåŠ›ã¨æ¤œç´¢å±¥æ­´")
    st.download_button("ğŸ’¾ çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", merged_df.to_csv(index=False).encode("utf-8"), file_name="meddra_results.csv", mime="text/csv")
else:
    st.info("ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆæ¬„ã«å…¥åŠ›ã—ã€[æ¤œç´¢å®Ÿè¡Œ]ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
