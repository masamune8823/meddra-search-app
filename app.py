import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import faiss

from helper_functions import (
    encode_query,
    search_meddra,
    rerank_results_v13,
    add_hierarchy_info,
    rescale_scores,
    predict_soc_category,
    format_keywords,
)

# ---------------- åˆæœŸè¨­å®š ---------------- #
st.set_page_config(page_title="MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª", page_icon="ğŸ”")
st.title("\U0001f50d MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")

# ---------------- ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---------------- #
@st.cache_resource
def load_assets():
    try:
        faiss_index = faiss.read_index("faiss_index.index")
    except Exception as e:
        st.error(f"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise e

    try:
        meddra_terms = np.load("meddra_terms.npy", allow_pickle=True)
        synonym_df = pickle.load(open("synonym_df_cat1.pkl", "rb"))
        term_master_df = pickle.load(open("term_master_df.pkl", "rb"))
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise e

    return faiss_index, meddra_terms, synonym_df, term_master_df

faiss_index, meddra_terms, synonym_df, term_master_df = load_assets()

# ---------------- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ---------------- #
query = st.text_input("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šçš®è†šãŒã‹ã‚†ã„ï¼‰", value="ã‚ºã‚­ã‚ºã‚­")
use_soc_filter = st.checkbox("GPTã«ã‚ˆã‚‹SOCäºˆæ¸¬ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰", value=True)

# ---------------- æ¤œç´¢å‡¦ç† ---------------- #
if st.button("æ¤œç´¢"):
    if not query.strip():
        st.warning("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è§£æä¸­..."):
            predicted_keywords = predict_soc_category(query)
            st.subheader("ğŸ§  GPTäºˆæ¸¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ•´å½¢å¾Œï¼‰")
            st.write(predicted_keywords)

        with st.spinner("FAISSã§ç”¨èªæ¤œç´¢ä¸­..."):
            search_results = []
            for kw in predicted_keywords:
                result = search_meddra(kw, faiss_index, meddra_terms, synonym_df, top_k=20)
                search_results.append(result)
            all_results = pd.concat(search_results).drop_duplicates(subset=["term"]).reset_index(drop=True)

        with st.spinner("å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ä¸­ï¼ˆGPTï¼‰..."):
            reranked = rerank_results_v13(query, all_results)
            reranked["score"] = rescale_scores(reranked["Relevance"].tolist())

        with st.spinner("éšå±¤æƒ…å ±ã‚’ä»˜åŠ ä¸­..."):
            st.write("åˆ—åãƒã‚§ãƒƒã‚¯ï¼ˆrerankedï¼‰:", reranked.columns.tolist())  # â† ã“ã“è¿½åŠ 
            final_results = add_hierarchy_info(reranked, term_master_df)

        if use_soc_filter:
            try:
                soc_prediction = predict_soc_category(query)
                final_results = final_results[final_results["SOC"].isin(soc_prediction)]
            except Exception as e:
                st.warning(f"ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        st.success("æ¤œç´¢å®Œäº†")

        expected_cols = ["term", "score", "HLT", "HLGT", "SOC"]
        available_cols = [col for col in expected_cols if col in final_results.columns]

        # è¡¨ç¤º
        st.dataframe(
              final_results[available_cols].rename(columns={"term": "ç”¨èª", "score": "ç¢ºã‹ã‚‰ã—ã• (%)"})
        )

        # CSVç”Ÿæˆæ™‚ã« encoding ã‚’æŒ‡å®šã™ã‚‹
        csv = final_results.to_csv(index=False, encoding="utf-8-sig")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        st.download_button("ğŸ“† çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="meddra_results.csv", mime="text/csv")
        
        # ğŸ” ãƒ†ã‚¹ãƒˆç”¨ãƒœã‚¿ãƒ³ï¼ˆâ† ã“ã“ãŒè¿½è¨˜éƒ¨åˆ†ï¼‰
        if st.button("ğŸ” ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚ºã‚­ã‚ºã‚­ï¼‰"):
            from test_meddra_full_pipeline import run_test_pipeline
            run_test_pipeline()
        # updated

