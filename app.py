# app.pyï¼ˆæœ€æ–°ç‰ˆãƒ»ä¿®æ­£æ¸ˆã¿ï¼‰
import os
import zipfile
import streamlit as st
import pandas as pd
import numpy as np
import pickle
import faiss
from utils import expand_query_gpt, encode_query, rerank_results_v13

# ğŸ”§ åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã®å¾©å…ƒãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def restore_split_file(output_path, parts, folder="."):
    with open(f"{output_path}.zip", "wb") as outfile:
        for part in parts:
            part_path = os.path.join(folder, f"{output_path}_{part}")
            if os.path.exists(part_path):
                with open(part_path, "rb") as infile:
                    outfile.write(infile.read())
            else:
                raise FileNotFoundError(f"{part_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# âœ… faiss_index ã®å¾©å…ƒã¨å±•é–‹
def restore_faiss_index_zip():
    zip_path = "faiss_index"
    parts = ["part_a", "part_b"]
    restore_split_file(zip_path, parts, folder=".")
    with zipfile.ZipFile(f"{zip_path}.zip", 'r') as zip_ref:
        zip_ref.extractall("data")

# âœ… search_assets.zip ã®å¾©å…ƒã¨å±•é–‹
def restore_search_assets():
    zip_path = "search_assets"
    parts = ["part_a", "part_b", "part_c", "part_d"]
    restore_split_file(zip_path, parts, folder=".")
    with zipfile.ZipFile(f"{zip_path}.zip", 'r') as zip_ref:
        zip_ref.extractall("data")

# âœ… meddra_embeddings.npy ã®å¾©å…ƒï¼ˆè§£å‡ä¸è¦ï¼‰
def restore_embeddings():
    output_path = "meddra_embeddings.npy"
    parts = ["part_a", "part_b"]
    restore_split_file("meddra_embeddings", parts, folder=".")

# âœ… å„ç¨®å¾©å…ƒã‚’å®Ÿè¡Œ
restore_faiss_index_zip()
restore_search_assets()
restore_embeddings()

# âœ… ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
def load_faiss_and_data():
    index = faiss.read_index("data/faiss_index.index")
    embeddings = np.load("data/meddra_embeddings.npy")
    with open("data/meddra_terms.npy", "rb") as f:
        terms = np.load(f, allow_pickle=True)
    with open("data/term_master_df.pkl", "rb") as f:
        master_df = pickle.load(f)
    return index, terms, master_df

faiss_index, meddra_terms, term_master_df = load_faiss_and_data()

# âœ… Streamlit UI
st.set_page_config(page_title="MedDRAæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")
st.title("ğŸ©º MedDRAæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—UIï¼‰")

st.header("1. åŒ»å¸«è¨˜è¼‰ç”¨èªã®å…¥åŠ›")
user_input = st.text_area("è‡ªç„¶è¨€èªã§è¨˜è¼‰ã•ã‚ŒãŸç—‡çŠ¶ã‚„å‡ºæ¥äº‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", height=100)

if st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ") and user_input:
    st.header("2. ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼‰")
    expanded_terms = expand_query_gpt(user_input)
    st.write("æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼š", expanded_terms)

    all_results = []
    for term in expanded_terms:
        query_vec = encode_query(term)
        scores, indices = faiss_index.search(np.array([query_vec]), k=20)
        for score, idx in zip(scores[0], indices[0]):
            result = {
                "term": meddra_terms[idx],
                "score": float(score),
                "query_term": term
            }
            all_results.append(result)

    reranked = rerank_results_v13(user_input, all_results)

    results_df = pd.DataFrame(reranked)
    merged_df = pd.merge(results_df, term_master_df, how="left", left_on="term", right_on="PT_English")
    merged_df = merged_df[["score", "term", "PT_Japanese", "HLT_Japanese", "HLGT_Japanese", "SOC_Japanese"]].copy()
    merged_df["ç¢ºã‹ã‚‰ã—ã•"] = (merged_df["score"] * 10).clip(0, 100).round(1).astype(str) + "%"
    merged_df.rename(columns={"term": "PT_English"}, inplace=True)
    merged_df = merged_df[["ç¢ºã‹ã‚‰ã—ã•", "PT_Japanese", "PT_English", "HLT_Japanese", "HLGT_Japanese", "SOC_Japanese"]]

    st.header("3. æ¤œç´¢çµæœï¼ˆTop10å€™è£œï¼‹ç¢ºã‹ã‚‰ã—ã•ï¼‰")
    st.dataframe(merged_df.head(10), use_container_width=True)

    st.header("4. å‡ºåŠ›ã¨æ¤œç´¢å±¥æ­´")
    st.download_button("ğŸ’¾ çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", merged_df.to_csv(index=False).encode("utf-8"), file_name="meddra_results.csv", mime="text/csv")

else:
    st.info("ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆæ¬„ã«å…¥åŠ›ã—ã€[æ¤œç´¢å®Ÿè¡Œ]ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
