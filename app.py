import streamlit as st
import pandas as pd
import pickle
import numpy as np
import faiss
import os
from helper_functions import expand_query_gpt, encode_query, rerank_results_v13, match_synonyms, merge_faiss_and_synonym_results

# zipãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å±•é–‹ï¼ˆstreamlit_app_bundle.zipï¼‰
if os.path.exists("streamlit_app_bundle.zip"):
    st.write("ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ä¸­...")
    unzip_log = os.popen("unzip -o streamlit_app_bundle.zip").read()
    st.text(unzip_log)

# ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
st.write("ğŸ“ ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:", os.getcwd())
st.write("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:", os.listdir())

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
@st.cache_resource
def load_data():
    terms = np.load("meddra_terms.npy", allow_pickle=True)
    embeddings_part_a = np.load("meddra_embeddings_part_a", allow_pickle=True)
    embeddings_part_b = np.load("meddra_embeddings_part_b", allow_pickle=True)
    embeddings = np.concatenate((embeddings_part_a, embeddings_part_b))

    with open("term_master_df.pkl", "rb") as f:
        term_master_df = pickle.load(f)
    with open("synonym_df_cat1.pkl", "rb") as f:
        synonym_df = pickle.load(f)

    return terms, embeddings, synonym_df, term_master_df

@st.cache_resource
def load_faiss_index():
    return faiss.read_index("faiss_index.index")

# UI
st.title("ğŸ’Š MedDRAæ¤œç´¢ã‚¢ãƒ—ãƒª")
st.write("ç—‡çŠ¶ã‚„è¨˜è¿°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
user_input = st.text_input("ç—‡çŠ¶å…¥åŠ›", "é ­ç—›")

if st.button("æ¤œç´¢"):
    with st.spinner("æ¤œç´¢ä¸­..."):
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            terms, embeddings, synonym_df, term_master_df = load_data()
            index = load_faiss_index()

            # ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆGPTï¼‰
            expanded_queries = expand_query_gpt(user_input)

            # å„ã‚¯ã‚¨ãƒªã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦FAISSæ¤œç´¢
            all_results = []
            for q in expanded_queries:
                query_vec = encode_query(q)
                D, I = index.search(np.array([query_vec]), k=10)
                for score, idx in zip(D[0], I[0]):
                    all_results.append({
                        "term": terms[idx],
                        "score": float(score),
                        "source": f"FAISS ({q})"
                    })

            # ã‚·ãƒãƒ‹ãƒ è¾æ›¸ã¨ã®ãƒãƒƒãƒ
            synonym_matches = match_synonyms(user_input, synonym_df)
            all_results.extend(synonym_matches)

            # å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            final_results = rerank_results_v13(all_results)

            # çµæœã®çµ±åˆãƒ»æ•´å½¢
            merged = merge_faiss_and_synonym_results(final_results, term_master_df)
            st.success("æ¤œç´¢å®Œäº†ï¼")
            st.dataframe(merged)

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
